{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7T_pPR93ODga"
   },
   "source": [
    "\n",
    "**Deploying Machine Learning Model on Streamlit Cloud**\n",
    "\n",
    "1.   Saving and Loading Sklearn Pipeline (Or Model)\n",
    "2.   Streamlit Introduction\n",
    "3.   Integrate Sklearn Model Into Streamlit\n",
    "4.   Push Inetegated Code to Github Repo\n",
    "5.   Deploy Github Repo into Streamlit Cloud\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AHAIlR3aNbfQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dByfwsU3Nb3d"
   },
   "source": [
    "**Streamlit Code**\n",
    "https://github.com/PradipNichite/sklearn_streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-UMWPmLZL9_-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JnV3hFRKMlp7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# ## For tokenization and data pre-processing \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "# ## fro cleaning the data \n",
    "import string\n",
    "import spacy\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "s756lXy7Ms-Z"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "OxwBXKKrMvlO",
    "outputId": "ea7cb253-0273-4028-b26c-2f61d0f4937e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0</td>\n",
       "      <td>Awesome, I'll see you in a bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4603</th>\n",
       "      <td>0</td>\n",
       "      <td>Hey j! r u feeling any better, hopeSo hunny. i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>0</td>\n",
       "      <td>No my mum went 2 dentist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>0</td>\n",
       "      <td>And you! Will expect you whenever you text! Ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>0</td>\n",
       "      <td>Like I made him throw up when we were smoking ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                            message\n",
       "278       0                     Awesome, I'll see you in a bit\n",
       "4603      0  Hey j! r u feeling any better, hopeSo hunny. i...\n",
       "1066      0                          No my mum went 2 dentist.\n",
       "3195      0  And you! Will expect you whenever you text! Ho...\n",
       "3341      0  Like I made him throw up when we were smoking ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\DSSGayathreeDevi\\\\Desktop\\\\surge_notebook\\\\NLP Module\\\\Spam Detector Component_1\\\\SMSSpamCollection.txt\", sep = \"\\t\", names=[\"label\", \"message\"])\n",
    "data['label'] = data['label'].map({'ham':0,'spam':1})\n",
    "data_1 = data[data['label']==1]\n",
    "data_0 = data[data['label']==0]\n",
    "sample_count = len(data[data['label']==1])\n",
    "data_0 = data_0.sample(sample_count, replace=True, random_state=1)\n",
    "data_balanced = pd.concat([data_0,data_1], axis=0)\n",
    "data = data_balanced\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "soq1B54gNSEm"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "Ubu3bbN7NKFA",
    "outputId": "5344b588-84a7-405a-e832-32baf9314cdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'by', 'sixty', 'formerly', 'at', 'afterwards', 'without', 'beside', 'hence', 'neither', 're', '’m', 'show', 'give', 'elsewhere', 'although', 'hers', 'which', 'been', 'nothing', 'and', 'yet', '‘ve', 'one', 'or', 'please', 'hereafter', 'it', 'since', 'nor', '‘re', 'often', 'more', 'herself', 'becomes', 'whereupon', 'my', 'four', 'though', 'becoming', 'anything', \"'d\", 'thus', 'you', 'latterly', 'unless', 'to', 'not', 'further', 'anyway', 'between', 'may', 'her', 'side', 'eleven', '‘ll', 'almost', 'yourself', 'part', 'name', 'using', 'can', 'be', 'really', 'due', 'other', 'anyhow', 'itself', 'has', 'just', 'nobody', 'had', 'seeming', 'make', 'once', 'through', 'see', 'therein', 'no', 'became', 'above', 'six', 'top', 'whereafter', 'yours', 'any', 'ten', 'others', 'go', 'herein', 'are', 'third', 'against', '‘s', 'but', 'twenty', 'mine', 'except', 'as', 'then', 'nevertheless', 'should', 'someone', 'whole', 'always', 'on', 'up', 'sometime', 'per', \"n't\", 'beyond', 'sometimes', 'your', 'thereby', 'fifteen', 'what', 'whoever', 'ours', 'bottom', 'perhaps', 'therefore', 'before', 'a', 'his', 'off', 'hereby', 'upon', 'whereas', 'forty', 'wherein', 'least', 'besides', 'empty', 'that', 'same', \"'re\", 'whereby', 'with', 'cannot', 'if', 'back', 'much', '’ll', 'move', 'used', 'full', 'seemed', 'whom', 'him', 'i', 'eight', 'was', 'across', 'indeed', 'front', 'another', 'behind', 'something', 'she', 'everything', 'put', 'thru', 'those', 'n’t', 'quite', 'into', '’s', 'amount', '‘m', 'first', 'is', 'call', 'become', 'than', 'ourselves', 'namely', 'nine', 'also', 'over', 'moreover', 'onto', 'could', 'whither', 'wherever', 'everywhere', 'somewhere', 'made', 'latter', 'who', 'anywhere', 'such', 'even', 'own', 'where', 'together', 'nowhere', 'hereupon', 'in', 'do', 'me', 'already', 'otherwise', 'anyone', 'we', 'around', 'noone', 'within', 'every', 'until', 'them', 'for', 'twelve', 'everyone', 'would', 'these', 'still', 'towards', 'next', 'fifty', 'various', 'were', 'being', 'whenever', 'keep', 'during', 'get', 'rather', 'its', 'whence', 'both', 'else', 'serious', 'have', 'of', 'myself', 'seem', 'out', 'several', 'whose', 'only', 'former', \"'m\", 'enough', 'why', 'while', 'so', 'this', 'some', 'few', 'n‘t', 'amongst', 'the', 'ca', 'about', \"'s\", 'how', 'most', 'two', 'very', 'take', 'along', 'he', 'must', 'themselves', 'doing', 'did', 'yourselves', 'here', 'whatever', 'thence', 'well', 'many', 'none', 'they', 'an', 'from', 'less', 'down', 'whether', 'ever', 'never', 'among', \"'ll\", 'somehow', 'however', 'via', 'three', 'alone', 'thereupon', 'himself', 'us', 'seems', 'beforehand', '’d', 'each', 'regarding', 'because', 'when', 'thereafter', 'under', 'their', '’ve', '’re', 'now', 'will', 'below', 'all', 'there', '‘d', 'again', 'after', 'too', 'hundred', 'am', 'meanwhile', 'throughout', 'might', \"'ve\", 'toward', 'five', 'done', 'does', 'our', 'last', 'mostly', 'say', 'either'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = nlp.Defaults.stop_words\n",
    "print(stop_words)\n",
    "punctuations = string.punctuation\n",
    "punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SLsl2bokNDt5"
   },
   "outputs": [],
   "source": [
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # print(doc)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() for word in doc ]\n",
    "\n",
    "    # print(mytokens)\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mTPlfRbkNIxo"
   },
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MX2PrV7ZNUlA"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['message'] # the features we want to analyze\n",
    "ylabels = data['label'] # the labels, or answers, we want to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "p7rUO7nqNZja"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyKSiP4KNcpk",
    "outputId": "d59cde16-bc14-4cc8-e838-92de4bc715ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(tokenizer=<function spacy_tokenizer at 0x00000200862F8280>)),\n",
       "                ('classifier', RandomForestClassifier())])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([ ('vectorizer', tfidf_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wfAdGje8Nd5I",
    "outputId": "b3a04306-bd0d-4407-ef7e-f833b92a80ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9364548494983278\n",
      "Logistic Regression Precision: 0.9927007299270073\n",
      "Logistic Regression Recall: 0.8831168831168831\n"
     ]
    }
   ],
   "source": [
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HFLoMVKjNjCo",
    "outputId": "aefbe90a-aa4b-47dc-ef73-9b99bb903c3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(['Free entry pass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kr6peFOFNlvC",
    "outputId": "95078451-225a-4594-b330-64e7b0fb1989"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(['win lottery'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3Oay9BwNnOF",
    "outputId": "7adcd095-402e-4d8c-cdd8-b66c5cc0bc3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(['I am waiting to get response from you'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkmvJqV8Nq3w"
   },
   "source": [
    "**Save and Load Sklearn Pipeline (Or Model)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "cLsROE2YNvPq"
   },
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DNlzI1b_Nwwp",
    "outputId": "4eb82cd8-746b-48e4-96d0-21005495a942"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipeline.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(pipe, 'pipeline.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "s2RlQUlwNyCS"
   },
   "outputs": [],
   "source": [
    "pipeline = load('pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYYjxzFO0L2n",
    "outputId": "7af19d9b-2050-480f-ee5f-cdc0ca9d505f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(tokenizer=<function spacy_tokenizer at 0x00000200862F8280>)),\n",
       "                ('classifier', RandomForestClassifier())])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2nFS_CdoNzPn",
    "outputId": "16012273-2bee-4f00-ed46-1ec975ce4182"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(['win lottery'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPfTSnfgN1Kn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Course Deploy Sklearn Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
